%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,brazil,oldfontcommands]{abntex2}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{float}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\docedilla}[2]{\underaccent{#1\mathchar'30}{#2}}
\newcommand{\cedilla}[1]{\mathpalette\docedilla{#1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[alf]{abntex2cite}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\usepackage{babel}
\begin{document}

\chapter{Introdução \label{sec:intro}}

Lorem ipsum dolor sit amet, soleat oblique inciderint mei at. Mea
harum legere delicata ut. Zril maiorum ea eum, affert adipiscing ad
per, veniam nostro sit in. Nec ea falli oratio expetendis. Alii impetus
prodesset mea ad, eum ei erroribus appellantur.

Eum etiam interpretaris ei, ad probo labores nam. Possit indoctum
est no, nulla persius elaboraret sed ne. Vide summo pertinacia ea
nec, et ius vocibus feugait. Dicat fuisset ex vix, dicta dolores pri
et. Ex tempor petentium consulatu cum, dico eius dicat no est, ea
cum liber virtute. Convenire posidonium mea in, pertinax scribentur
eloquentiam eam ei, usu et labore dictas complectitur.

Ad sit eruditi dolorum, purto omittantur vix ut. Mel ad fugit adolescens,
et nisl oblique molestiae pri, in incorrupte elaboraret vim. Vis erat
praesent in, pri ad eros pertinax. Ex adhuc meliore delicatissimi
cum, ut ius justo nulla, in ferri ceteros nam.

At mei quas admodum appellantur, cum ex dolores intellegam, sale soluta
probatus ea eum. Rebum bonorum in pri, ad nam graeco insolens, ex
esse facete has. Qui ei elit referrentur, eum alii scripta ne. Ei
ius tantas mucius latine, justo percipitur cu ius, eum dicam mollis
iisque eu. Nam dico tota splendide cu, pri an omittam dignissim voluptatum.
Aeque tacimates consulatu eu usu, an pro pertinax electram.

Iudicabit corrumpit scribentur ad est, appareat vituperatoribus cu
vix, case movet at quo. Dolorum maiestatis disputando no sit. At etiam
dolorem usu, lorem summo quo an, pro dolore nusquam quaerendum eu.
Eos suas mazim invenire et, accusam fabellas id nam, te eum doctus
consequuntur. Conceptam elaboraret ad nec, est in etiam minimum fabella.


\chapter{Objetivos \label{sec:objectives}}


\section{Objetivos Gerais \label{sec:objectives:general_objectives}}

Desenvolver uma metodologia computacional que análise um exame termográfico
dinâmico das mamas, a fim de encontrar padrões que diferenciem mamas
saudáveis de mamas doentes. 

A principal motivação do trabalho é ajudar médicos especialistas no
diagnóstico precoce do câncer de mama, portanto qualquer informação
que possa discriminar mamas saudáveis de mamas doentes é relevante.


\section{Objetivos Específicos \label{sec:objectives:specific_objectives}}

Alguns objetivos extras são necessários para completar os objetivos
gerais, sendo eles:
\begin{itemize}
\item Desenvolver técnicas que permitam a correção de postura em exames
termográficos dinâmicos. Tais técnicas são conhecidas como registro
de imagens, onde o resultado final pode variar muito de uma técnica
utilizada para outra. Portanto, deve-se utilizar a que melhor se encaixe
no problema.
\item Analisar a variação de temperatura entre os diferentes tempos do exame,
gerando características que sejam capazes de diferenciar os exames
de pacientes saudáveis de exames de pacientes doentes.
\item Utilizar as características extraídas para gerar um classificador
automático, que seja capaz de classificar corretamente novos exames.
\end{itemize}

\chapter{Fundamentação Teórica \label{sec:fundamentation}}

Nesta seção, serão abordados os conceitos necessários para o entendimento
da metologia proposta.


\section{A Termografia \label{sub:fundamentation:termography}}

A termografia é uma técnica que permite a visualização dos raios do
espectro infravermelho de forma a mapear a temperatura de um objeto.
A termografia infravermelha da mama é um tipo de exame que detecta
a radiação infravermelha emitida pela superfície da mama produzindo
um mapa de temperatura conhecido como termograma. A grande vantagem
deste tipo de exame em relação aos exames mais conhecidos, como a
mamografia, é que este não utiliza radiação no processo de obtenção
das imagens e também não causa incômodo físico ao paciente, pois não
é necessário a compressão das mamas, como ocorre na termografia. Outro
fator a se levar em consideração é o custo extremamente baixo e o
fácil manuseio do equipamento necessário para obtenção do exame \cite{borchartt2013breast}.
A Figura \ref{fig:fundamentation:thermography:breast-thermography}
apresenta uma termografia em pseudocor, onde a temperatura varia de
acordo com a paleta de cores à direita da imagem.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.25]{figures/fundamentation/termography/termography}\caption{Termografia mamária.\label{fig:fundamentation:thermography:breast-thermography}}

\par\end{centering}



\end{figure}



\subsection{Termografia Estática\label{sub:fundamentation:termography:static}}

A termografia estática (TI), é o tipo de termografia onde a medição
de temperatura é feita uma única vez. No caso de TI das mamas são
procedimentos \cite{silva2014new} de preparação do ambiente e cuidados
extras ao paciente para que o exame tenha resultado satisfatório,
pois vários fatores podem influenciar o resultado final \cite{lincoln_thesis2015}. 


\subsection{Termografia Dinâmica}

A termografia dinâmica (TID), é um tipo de termografia onde a mediação
de temperatura é realizada através de várias etapas de tempo, ou seja,
TID mede as mudanças de temperatura sobre um determinado periodo de
tempo. Originalmente conceituada por \cite{anbar1987computerized},
onde notou-se que mudanças bruscas na temperatura da pele produziam
informações valiosas, que não podem ser obtidos por meio de uma TI.

A TID apresenta uma característica importante na detecção de lesões
mamárias, pois áreas saudáveis apresentam um comportamento distinto
de áreas com algum tipo de patologia. Neoplásias são associadas com
a angiogênese que causa um aumento de vascularização na região, sendo
que, os vasos recém formados apresentam a características de possuir
poucas terminações nervosas, o que causa um comportamento irregular
a estimulos externos, que é detectável através de um exame de termografia
dinâmico \cite{lincoln_thesis2015}.

Na prática a TID monitora as mudanças na temperatura da pele, onde
o estimulo térmico através de uma corrente de ar produz um contraste
entre tecidos saudáveis e doentes. Regiões saudáveis da mama tendem
a apresentar uma diminuição de temperatura com o estímulo de ar, enquanto
as regiões doentes tendem a permanecer estáticas \cite{amalu2004nondestructive}. 


\section{Registro de Imagens\label{sec:fundamentation:image_registration}}

Quando imagens que foram extraídas de diferentes tempos, pontos de
vista ou aparelhos precisam ser comparadas, ocorre um problema de
alinhamento das coordenadas dessas imagens. É necessário efetuar alguma
forma de processamento para que essas imagens possam ser comparadas
adequadademente .\emph{ }O \emph{Registro de Imagens }é uma técnica
que utiliza uma transformação $T$ para mapear a posição e o valor
de intensidade de um pixel $p$ da imagem $A$ para $q$ na imagem
$B$, como pode ser visto pela Equação \ref{eq:fundamentation:image_registration:transform_eq}.
Figura\ref{fig:fundamentation:image_registration:mapping} mostra
o efeito desse mapeamento. 

\begin{equation}
T:p\rightarrow q\Leftrightarrow T(p)=q\label{eq:fundamentation:image_registration:transform_eq}
\end{equation}


\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/mapping}
\par\end{centering}

\caption{Registro de imagens é a tarefa de achar uma transformação espacial
de uma imagem em outra. \label{fig:fundamentation:image_registration:mapping}}



\fonte{\cite{ibanez2003itk}}
\end{figure}


Na literatura, é possível encontrar diversas definições para o problema
de registro de imagens. De acordo com \cite{brown1992survey}, o processo
de registro de imagens é transformação de conjuntos distintos de dados
para um mesmo sistema de coordenadas. Em \cite{crum2014non} é definido
como um processo que determina correspondências entre as características
de imagens extraídas em diferentes momentos, pontos de vista ou aparelhos.
Tais correspondências podem ser utilizadas para aplicar transformações
(rotação, translação, alongamento, etc.) em uma imagem de forma que
seja possível a comparação entre as duas imagens. A forma mais intuitiva
de utilização do registro é para corrigir diferenças na posição entre
scans . Registro de imagens adiciona valor à imagens, permitindo que
imagens estruturais (CT, MR, ultrasom) e funcionais (PET, SPECT, functional
MRI) sejam vistas e analisadas no mesmo sistema de coordenadas, e
facilita o uso de novas imagens, como para monitorar e quantificar
a evolução de uma doença à medida que o tempo passa \cite{crum2014non}.

A Figura \ref{fig:fluxo-registro} mostra o fluxograma básico de um
processo de registro de imagens. O conjunto de entrada básico de um
processo de registro de imagens é composto por:
\begin{itemize}
\item Imagem fixa: imagem estática cujo espaço de coordenadas é o objetivo.
\item Imagem móvel: imagem que será transformada para o espaço de coordenadas
da imagem fixa.
\item Transformação: função que irá ser responsável por mapear os pixels
da imagem móvel na imagem fixa. Geralmente é responsável por dar nome
ao registro.
\item Métrica: uma medida que indica o quanto duas imagens são equivalentes.
\item Interpolador: uma técnica para interpolar os valores da imagem móvel
quando são remostrados através da transformação.
\item Otimizador: o método utilizador para achar os melhores parâmetros
da transformação que otimizam a métrica entres as duas imagens. 
\end{itemize}
\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/fluxo}
\par\end{centering}

\begin{centering}
\caption{Fluxograma do processo de registro de imagens. \label{fig:fluxo-registro}}

\par\end{centering}


\fonte{\cite{parraga2008atlas}}
\end{figure}



\subsection{Métricas de similaridade}

As métricas de similaridade são provavelmente o elemento mais crítico
no problema do registro de imagens, pois define o objetivo final do
processo que é medir o quanto a imagem móvel é equivalente à imagem
fixa após a transformação ser aplicada. 


\subsubsection{Correlação Cruzada Normalizada}

A Correlação Cruzada Normalizada é uma métrica insensível à fatores
multiplicativos entre as imagens. Produz uma função de custo com picos
afiados e mínimos bem definidos. Por outro lado tem um raio de captura
relativamente pequeno. Sua aplicação é limitada a imagens de mesma
modalidade (adquiridas com o mesmo tipo de aparelho) \cite{ibanez2003itk}.
A correlação cruzada normalizada é dada pela Equação \ref{eq:fundamentation:image_registration:normalized_cross_correlation}:

\begin{equation}
C(f,m)=-1\cdot\frac{\sum_{i=1}^{N}(f_{i}\cdot m_{i})}{\sqrt{\sum_{i=1}^{N}f_{i}^{2}\cdot}\sum_{i=1}^{N}m_{i}^{2}},\label{eq:fundamentation:image_registration:normalized_cross_correlation}
\end{equation}
onde $f$ e $m$ são os valores de pixels em forma de vetor das imagens
fixa e móvel, respectivamente, $i$ indica a posição do pixel em $f$
e $m$, $N$ é a quantidade pixels a ser considerado. Note que o a
equação é multiplicada por $-1$. Esse fator é responsável por fazer
o otimizador procurar os valores que mais se aproximem de $0$, que
é quando duas imagens são ditas equivalentes.


\subsection{Transformação}

A transformação é responsável por mapear os pixels da imagem móvel
para a imagem fixa. Podemos dividir as transformações em dois conjuntos
distintos, rígidas e deformáveis. 

As transformações rígidas aplicam sobre a imagem móvel apenas transformações
simples como operações de translação e rotação, dessa forma a imagem
móvel não sofre deformações, sofrendo apenas o alinhamento de suas
coordenadas. Registros que utilizam transformações rígidas são chamados
de regístro rígidos e na prática não são utilizados individualmente,
pois não são capazes de corrigir erros locais que necessitam de transformações
mais complexas para serem resolvidas. Apesar disso, são bastante utilizados
como pré-processamento para os chamados registros deformáveis. 

As transformações deformáveis são capazes de efetuar deformações nas
imagens, dessa forma é possível corrigir erros que uma transformação
rígida não é capaz de corrigir. Registro de imagens que utilizam esse
tipo de transformação são geralmente chamados de registros não-rígidos
ou deformáveis. 


\subsubsection{Translação}

A translação é uma transformação rígida bastante simples. Ela mapeia
todos os pontos da imagem móvel adicionando um vetor à eles. Dado
um ponto 2D $p=(x,y)$, e o vetor de translação $\vec{t}=(t_{x},t_{y})$,
podemos dizer que o ponto transladado $q$ é dado por 

\begin{equation}
q=p+\vec{t}=(x+t_{x},y+t_{y}).\label{eq:fundamentation:image_registration:transform:translate}
\end{equation}


Utilizando a notação de matrizes em coordenadas homogêneas temos

\begin{equation}
q=\begin{bmatrix}x'\\
y'\\
1
\end{bmatrix}=\begin{bmatrix}1 & 0 & t_{x}\\
0 & 1 & t_{y}\\
0 & 0 & 1
\end{bmatrix}\cdot\begin{bmatrix}x\\
y\\
1
\end{bmatrix}.\label{eq:fundamentation:image_registration:transform:translate_matrix}
\end{equation}


A Figura \ref{fig:fundamentation:image_registration:transform:translate}
demonstra o resultado da transformação de translação. Vemos que apenas
as posições dos pixels foram alteradas.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/translate}
\par\end{centering}

\caption{Efeito da translação.\label{fig:fundamentation:image_registration:transform:translate}}



\fonte{Elaborada pelo autor}
\end{figure}



\subsubsection{Rotação}

A transformação é de rotação é uma transformação rígida capaz de rotacionar
os objetos a partir de um centro de rotação. Dado um ponto $p=(x,y)$
e um ângulo de rotação $\theta$ em radianos, o ponto $q$ é o resultado
da rotação de $p$ em $\theta$ radianos dado por

\begin{equation}
q=\begin{bmatrix}x'\\
y'\\
1
\end{bmatrix}=\begin{bmatrix}cos\,\theta & -sen\,\theta & 0\\
sen\,\theta & cos\,\theta & 0\\
0 & 0 & 1
\end{bmatrix}.\label{eq:fundamentation:image_registration:transform:rotation}
\end{equation}


A Figura \ref{fig:fundamentation:image_registration:transform:rotation}
representa uma transformação de 90º em torno da origem dos eixos.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/rotation}
\par\end{centering}

\caption{Rotação de $\theta=\frac{\pi}{2}=90\text{º}.$\label{fig:fundamentation:image_registration:transform:rotation}}
\end{figure}


Geralmente em registro de imagens, a translação e rotação são aplicadas
em conjunto, para que a rotação não altere a posição do objeto. Dessa
forma, aplicamos uma operação de translação para trazer o objeto para
a origem das coordenadas, aplicamos a rotação, então aplicamos uma
translação inversa para levar o objeto de volta para sua posição inicial.


\subsubsection{Escala}

A transformação de escala é uma transformação não-rígida capaz alterar
a forma de um objeto. Dado um ponto $p=(x,y)$ e o fator de escala
$s=(s_{x},s_{y})$, podemos dizer que o ponto $q$ é dado por 

\begin{equation}
q=p\cdot s=(x\cdot s_{x},y\cdot s_{y}).\label{eq:fundamentation:image_registration:transform:scale}
\end{equation}


Em notação de matrizes homogêneas temos

\begin{equation}
q=\begin{bmatrix}x'\\
y'\\
1
\end{bmatrix}=\begin{bmatrix}s_{x} & 0 & 0\\
0 & s_{y} & 0\\
0 & 0 & 1
\end{bmatrix}\cdot\begin{bmatrix}x\\
y\\
1
\end{bmatrix}.\label{eq:fundamentation:image_registration:transform:scale_matrix}
\end{equation}


Na Figura \ref{fig:fundamentation:image_registration:transform:scale}
vemos o efeito da transformação de escala. Caso os parâmetros sejam
iguais $s_{x}=s_{y}$, não mudamos a forma do objeto. Porém, caso
$s_{x}\neq s_{y}$, temos uma transformação que altera a forma do
objeto transformado.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/scale}
\par\end{centering}

\caption{Transformação de escala.\label{fig:fundamentation:image_registration:transform:scale}}



\fonte{Elaborada pelo autor}
\end{figure}



\subsubsection{Transformações FFD baseadas em \emph{B-Splines }cúbicas}

Um dos tipos de transformações não-rígidas mais comuns atualmente
são as chamadas transformações FFD (deformação de forma livre) baseadas
em \emph{b-splines} cúbicas \cite{yin2009mass}. A ideia básica desse
tipo de transformação é manipular a grade de pixels a partir dos chamados
pontos de controle. Essa manipulação permite criar um campo de deslocamento,
que irá mapear os pixels da imagem móvel. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/displacement}
\par\end{centering}

\caption{O conceito de campo de deslocamento. Para cada posição de pixel na
imagem móvel, existe uma direção e distância que ele precisa se mover
para se assemelhar à imagem fixa. }



\fonte{\cite{schwarz2007non}}
\end{figure}


Seja $\Phi$ a grade uniforme de dimensões $n_{x}\times n_{y}\times n_{z}$
com $\Phi_{i,j,k}$ representando o deslocamento do $ijk$-ésimo ponto
de controle. O espaço entre as grandes controle nas direções $x$,
$y$ e $z$ são denotadas por $\delta_{x}$, $\delta_{y}$e $\delta_{z}$,
respectivamente. A transformação $T(\text{{x}}:\Phi)$ é definida
por 

\begin{equation}
T(\text{{x}}:\Phi)=\sum_{l=0}^{3}\sum_{m=0}^{3}\sum_{n=0}^{3}\beta_{l}(u)\beta_{m}(v)\beta_{n}(w)\Phi_{i+l,j+m,k+n},\label{eq:fundamentation:image_registration:ffd_bspline:transform}
\end{equation}
onde os parâmetros são dados por

\begin{equation}
i=|\frac{x}{\delta_{x}}|-1,\label{eq:fundamentation:image_registration:ffd_bspline:p1}
\end{equation}


\begin{equation}
j=|\frac{y}{\delta_{y}}|-1,\label{eq:fundamentation:image_registration:ffd_bspline:p2}
\end{equation}


\begin{equation}
z=|\frac{z}{\delta_{z}}|-1,\label{eq:fundamentation:image_registration:ffd_bspline:p3}
\end{equation}


\begin{equation}
u=\frac{x}{\delta_{x}}-(i+1),\label{eq:fundamentation:image_registration:ffd_bspline:p4}
\end{equation}


\begin{equation}
v=\frac{y}{\delta_{y}}-(j+1),\label{eq:fundamentation:image_registration:ffd_bspline:p5}
\end{equation}


\begin{equation}
w=\frac{z}{\delta_{z}}-(z+1).\label{eq:fundamentation:image_registration:ffd_bspline:p6}
\end{equation}
As funções $\beta$ são \emph{B-splines }cúbicas e definidas por

\begin{equation}
\beta_{0}(t)=(-t^{3}+3t^{2}-3t+1)/6,\label{eq:fundamentation:image_registration:ffd_bspline:b0}
\end{equation}


\begin{equation}
\beta_{1}(t)=(3t^{3}-6t^{2}+4)/6,\label{eq:fundamentation:image_registration:ffd_bspline:b1}
\end{equation}


\begin{equation}
\beta_{2}(t)=(-3t^{3}+3t^{2}+3t+1)/6,\label{eq:fundamentation:image_registration:ffd_bspline:b2}
\end{equation}


\begin{equation}
\beta_{3}(t)=t^{3}/6,\label{eq:fundamentation:image_registration:ffd_bspline:b3}
\end{equation}
onde $0\le t\le1$. 

Um parâmetro muito importante desse tipo de transformação é a resolução
da grade de controle, pois a partir dela serão gerados os campos de
deslocamentos. Uma grade muito espaçosa permite a modelagem de transformações
deformáveis globais, enquanto uma grade mais fina modela deformações
altamente locais \cite{yin2009mass}.


\subsection{Interpolador}

No processo do registro, a métrica geralmente compara os valores de
intensidade dos pixels da imagem fixa com os pixels correspondentes
na imagem móvel transformada. Quando transformamos um ponto de um
espaço para o outro através de uma transformação, este geralmente
irá ser mapeado para uma posição fora da grade de pixels da imagem
(Figura \ref{fig:fundamentation:image_registration:grid}) . A função
do interpolador é calcular o valor de intensidade em uma dada posição
de forma correta \cite{ibanez2003itk}.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/image_registration/ITKSoftwareGuide-Book2259x}
\par\end{centering}

\caption{Posições de grade da imagem fixa mapeadas para posições fora da grade
na imagem móvel.\label{fig:fundamentation:image_registration:grid}}



\fonte{\cite{ibanez2003itk}}
\end{figure}



\subsubsection{Interpolador Linear}

O interpolador linear assume que os valores de intensidade dos pixels
variam de forma linear entre as posições de grade. Dessa forma, os
valores interpolados serão contínuos espacialmente, porém o gradiente
de intensidade será descontinuo na grade. Se dois pontos conhecidos
são dados por $(x_{0},y_{0})$ e $(x_{1},y_{1})$ a interpolação linear
é dada pela Equação \ref{eq:fundamentation:image_registration:linear_interpolation}.

\begin{equation}
y=y_{0}+(y_{1}-y_{0})\frac{x-x_{0}}{x_{1}-x_{0}}\label{eq:fundamentation:image_registration:linear_interpolation}
\end{equation}



\subsection{Otimizador}

Como dito anteriormente, o papel do otimizador é encontrar os melhores
parâmetros para a transformação escolhida no processo de registro
de imagens. Dessa forma, um amplo conjunto de otimizadores podem ser
utilizados, como os \emph{Algoritmos Genéticos \cite{whitley1994genetic}
}e o \emph{Gradiente Descendente} \cite{burges2005learning}.


\subsubsection{LBFGS}

O \emph{LBFGS }(Limited-memory Broyden-Fletcher-Goldfarb-Shanno) é
um método de otimização bastante comum em problemas de registro de
imagens. De acordo com \cite{sheppard2008optimization}, é um método
quasi-Newton que constrói informações sobre as segundas derivadas
durante a otimização e utiliza essa informação para avançar em direção
ao minimo harmônico previsto. Mais especificamente, a inversa da matriz
hessiana $H^{-1}$ é construída interativamente, começando a partir
da matriz diagonal. O método pode ser utilizado de duas formas. Na
primeira, uma direção de busca,

\begin{equation}
d_{j}=F_{j}H_{j}^{-1},\label{eq:fundamentation:image_registration:optimizator:lbfgs:first:dir}
\end{equation}
é identificada em cada iteração, e a minimizador de linha é utilizado
para avançar na direção,

\begin{equation}
R_{j+1}=R_{j}+\lambda d_{j}.\label{eq:fundamentation:image_registration:optimizator:lbfgs:first:dir2}
\end{equation}


A segunda forma é utilizar $H^{-1}$ diretamente para calcular o avanço,

\begin{equation}
R_{j+1}=R_{j}+F_{j}H^{-1}.\label{eq:fundamentation:image_registration:optimizator:lbfgs:first:dir3}
\end{equation}


O LBFGS utiliza a memória da iteração anterior para construir $H^{-1}$.
O número de iterações é um parâmetro variável e é definido pelo usuário.


\section{Matrizes de Coocorrência de Níveis de Cinza\label{sec:fundamentation:glcm}}

A análise de textura em imagens é uma técnica importante na identificação
de características em imagens digitais. Uma das primeiras técnicas
utilizadas para a extração dessas características foram as matrizes
de coocorrências de níveis de cinza também chamadas de \emph{GLCMs
}(Gray Level Co-ocurrence Matrix) originalmente propostas em \cite{haralick1973textural}.
Desde então elas vem sendo bastante utilizadas em várias aplicações
de análise de textura e permanecem sendo uma ferramenta importante
no domínio de análise de texturas \cite{sebastian2012gray}.

A GLCM é uma técnica que utilizada para extrair características estatísticas
de textura de segunda ordem \cite{albregtsen2008statistical}, ou
seja, a medição considera a relação entre pares de pixels, geralmente
vizinhos \cite{glcm_tutorial}. 


\subsection{Construção de uma GLCM}

Seja $I$ uma imagem em níveis de cinza e$N$ a quantidade de níveis
de cinza, uma matriz de coocorrência $G$ é uma matriz quadrada de
ordem $N$ \cite{sebastian2012gray}. O elemento da matriz $P(i,j|\Delta x,\Delta y)$
é a frequência relativa em que dois pixels de intensidade $(i,j)$,
separados por uma distância $(\Delta x,\Delta y)$, são vizinhos.
Também é possível dizer que o elemento $P(i,j|d,\theta)$ contém os
valores da probabilidade de segunda ordem para mudanças entre os níveis
de cinza $i$ e $j$ em uma distância $d$ e um ângulo $\theta$ \cite{albregtsen2008statistical}.

Seja $I$ uma imagem de dimensões $WxH$, então uma GLCM de ir pode
ser definida pela Equação \ref{eq:glcm:def}.

\begin{equation}
G(i,j|\Delta x,\Delta y)=\sum_{p=1}^{W}\sum_{q=1}^{H}A\label{eq:glcm:def}
\end{equation}


\begin{equation}
A=\begin{cases}
1,\:se\:I(p,q)=i\:e\:I(p+\Delta x,q+\Delta y)=j\\
0,\:caso\:contr\acute{a}rio
\end{cases}\label{eq:glcm:case}
\end{equation}


Como exemplo temos uma imagem $I$ de dimensões $5x5$, com $4$ níveis
de cinza. Então as matriz de co-ocorrência $G(i,j|\Delta x=1,\Delta y=0)$
e $G(i,j|\Delta x=1,\Delta y=1)$ são representadas abaixo.

\[
I=\begin{bmatrix}2 & 0 & 2 & 1 & 1\\
2 & 1 & 2 & 3 & 1\\
3 & 1 & 3 & 2 & 0\\
0 & 0 & 1 & 2 & 2\\
3 & 3 & 3 & 1 & 1
\end{bmatrix}
\]


\[
G(i,j|\Delta x=1,\Delta y=0)=\begin{bmatrix}1 & 1 & 1 & 0\\
0 & 2 & 2 & 1\\
2 & 2 & 1 & 1\\
0 & 3 & 1 & 2
\end{bmatrix}
\]


\[
G(i,j|\Delta x=1,\Delta y=1)=\begin{bmatrix}0 & 0 & 1 & 2\\
0 & 3 & 0 & 1\\
0 & 3 & 2 & 1\\
2 & 0 & 1 & 0
\end{bmatrix}
\]



\subsection{GLCM Normalizada}

Considere $N=\sum_{i}\sum_{j}G_{d}(i,j)$, que é a quantidade coocorrências
de pares em $G_{d}$. Seja $GN_{d}(i,j)=\frac{1}{N}G_{d}(i,j)\cdot GN_{d}$,
chamada de matriz de coocorrência normalizada, onde as entradas $(i,j)$
de $GN_{d}(i,j)$ são as probabilidades de coocorrência de um determinado
pixel com intensidade $i$ ser separado de um pixel de intensidade$j$
por uma distância $k$ em uma determinada direção $d$ \cite{sebastian2012gray}.


\subsection{Características de Textura}

A partir de uma GLCM, é possível extrair um conjunto de características
que descrevem a textura de determinada imagem \cite{albregtsen2008statistical},
disponíveis em \cite{haralick1973textural,haralick1979statistical,conners1984segmentation}.
De acordo com \cite{glcm_tutorial} temos as seguintes características
que podem ser extraídas a partir de uma GLCM:

\begin{equation}
Constraste=\sum_{i,j=0}^{N-1}P_{i,j}(i-j)^{2}\label{eq:fundamentation:glcm:contraste}
\end{equation}


\begin{equation}
Dissimilaridade=\sum_{i,j=0}^{N-1}P_{i,j}|i-j|\label{eq:fundamentation:glcm:dissimilaridade}
\end{equation}


\begin{equation}
Homogeineidade=\sum_{i,j=0}^{N-1}\frac{P_{i,j}}{1+(i-j)^{2}}\label{eq:fundamentation:glcm:homogeneidade}
\end{equation}


\begin{equation}
ASM=\sum_{i,j=0}^{N-1}P_{i,j}^{2}\label{eq:fundamentation:glcm:asm}
\end{equation}


\begin{equation}
Energia=\sqrt{ASM}\label{eq:fundamentationg:glcm:energia}
\end{equation}


\begin{equation}
Correla\cedilla{c}\tilde{a}o=\sum_{i,j=0}^{N-1}P_{i,j}\left[\frac{(i-\mu_{i})\cdot(j-\mu_{j})}{\sqrt{(\sigma_{i}^{2})\cdot(\sigma_{j}^{2})}}\right]\label{eq:fundamentationg:glcm:correla=0000E7=0000E3o}
\end{equation}


\begin{equation}
\mu_{i}=\sum_{i,j=0}^{N-1}i\cdot(P_{i,j})\label{eq:fundamentationg:glcm:media_i}
\end{equation}


\begin{equation}
\mu_{j}=\sum_{i,j=0}^{N-1}j\cdot(P_{i,j})\label{eq:fundamentationg:glcm:media_j}
\end{equation}


\begin{equation}
\sigma_{i}^{2}=\sum_{i,j=0}^{N-1}P_{i,j}(i-\mu_{i})^{2}\label{eq:fundamentationg:glcm:variantion_i}
\end{equation}


\begin{equation}
\sigma_{j}^{2}=\sum_{i,j=0}^{N-1}P_{i,j}(j-\mu_{j})^{2}\label{eq:fundamentationg:lcm:variation_j}
\end{equation}


onde $N$ é quantidade de níveis de cinza $P_{i,j}$é a frequência
de um par de pixels $i$ e $j$ serem vizinhos. 


\section{Redução de Dimensionalidade\label{sec:fundamentation:dimensionality_reduction}}

Em \emph{Aprendizado de Máquina}, geralmente possuímos um conjunto
de dados previamente disponíveis para utilizar em um classificador
qualquer. Nesse conjunto de dados, cada indivíduo é representado por
um conjunto de características extraídas do dado original. Por exemplo,
em imagens de níveis de cinza é possível extrair as características
representadas pelas Equações \ref{eq:fundamentation:glcm:contraste},\ref{eq:fundamentation:glcm:dissimilaridade},\ref{eq:fundamentation:glcm:homogeneidade},\ref{eq:fundamentation:glcm:asm},\ref{eq:fundamentationg:glcm:energia}e\ref{eq:fundamentationg:glcm:correla=0000E7=0000E3o}
a partir de um GLCM qualquer. A quantidade de características que
um indivíduo possui é o que chamamos de \emph{dimensão}. Para um conjunto
de dados de dimensão $D=6$, então temos $6$ características que
descrevem cada indivíduo. 

A fácil pensar que quanto maior o número de características, melhor
para discriminar os indivíduos. Porém, um conjunto de dados com uma
dimensão muito alta implica maior complexidade no problema de classificação.
Outro problema é que, em muitos casos, nem todas as características
são consideradas importantes para o entendimento de um certo problema\cite{fodor2002survey}.

Em termos matemáticos, o problema de \emph{Redução de Dimensionalidade}
pode ser descrito da seguinte forma: Dado um conjunto dados com $p$
dimensões $x=(x_{1},x_{2},...,x_{p})^{T}$, encontrar uma representação
de menor dimensão $s=(s_{1},s_{2},...,s_{k})^{T}$, onde $k\le p$
, que seja capaz de capturar o conteúdo dos dados originais, de acordo
com um critério preestabelecido \cite{fodor2002survey}.

Algumas técnicas de redução de dimensionalidade são descritas abaixo\emph{.}


\subsection{Seleção de características univariada}

A seleção de características univariada seleciona as melhores características
baseada em testes estatísticos univariados \cite{scikit-learn-feat}.
Por exemplo, podemos selecionar as $K$ melhores características através
de um teste estatístico que classifique as características que melhor
separam as classes de índividuos de um conjunto de dados. 


\subsection{\emph{Principal Component Analysis}}

O \emph{Principal Component Analysis }(PCA) é uma técnica de redução
linear \cite{jackson2005user}. Por ser baseada na matriz de covariância
dos dados, é um método de segunda ordem. O PCA reduz a dimensionalidade
encontrando os \emph{principais componentes }(PCs) do conjunto de
dados com maior variância, que são vetores formados pelas combinações
das características originais. Por exemplo, o primeiro PC é o que
possui maior variância, Temos $s_{1}=x^{T}w_{1}$, onde o vetor de
coeficientes de $p$ dimensões é $w_{1}=(w_{1,1},...,w_{1,p})^{T}$
que resolve: 

\begin{equation}
w_{1}=arg\:max_{||w=1||}Var\{x^{T}w\}\label{eq:fundamentation:reduction:pca}
\end{equation}


O segundo PC é a combinação linear com a segunda maior variância e
ortogonal ao primeiro PC, e assim sucessivamente. Existem tantos PCs
quanto o número de características originais \cite{fodor2002survey}. 

Geralmente, normaliza-se os dados antes da aplicação do PCA, pois
o método é dependente da escala. Um dos métodos possíveis para a normalização
é escalar cada características entre 0 e 1. Então, assumindo que os
dados estão normalizados, podemos calcular a matriz de covariância

\begin{equation}
C=\frac{1}{p-1}\sum_{i=1}^{p}(x_{i}-\bar{x})\cdot(x_{i}-\bar{x})^{T},\label{eq:fundamentation:reduction:pca:cov}
\end{equation}
onde $\bar{x}$é o vetor médio dado por

\begin{equation}
\bar{x}=\frac{1}{p}\sum_{i=1}^{p}x_{i}\label{eq:fundamentation:reduction:pca:mean_vector}
\end{equation}


Utilizando a relação dos autovetores e autovalores

\begin{equation}
Cv=\lambda v,\label{eq:fundamentation:reduction:pca:eigen}
\end{equation}


onde $v$ é um autovetor de $C$ e $\lambda$o seu respectivo autovalor.
Cada autovetor corresponde a um PC, e os respectivos autovalores indicam
o grau de variância de cada PC. Assim, escolhendo os $k$ PCs com
maior variância absoluta, podemos mapear nossos dados originais em
um novo conjunto 

\begin{equation}
S=W^{T}X,\label{eq:fundamentation:reduction:pca:result}
\end{equation}
onde $X$ é o conjunto de dados originais e $W$ uma matriz de transformação

\begin{equation}
W=(v_{1},...v_{k}),\label{eq:fundamentation:reduction:pca:transform}
\end{equation}
onde $v_{1}$é o PC de maior variância e $v_{k}$ o k-ésimo PC de
maior variância.

É uma técnica muito útil quando queremos transformar nossas características
originais, onde alguma delas não oferecem muita variação nos dados,
em um espaço novo com menor dimensão onde as novas características
possuem uma melhor dispersão nos dados. A Figura \ref{fig:fundamentation:reduction:pca:samples}
mostra um conjunto de índividuos gerados por distribuições normais
multivariadas com 3 características. Utilizando o PCA para reduzir
o espaço dimensional para apenas 2, temos a Figura \ref{fig:fundamentation:reduction:pca:samples_transformed},
que exemplifica o efeito da redução. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/dimensionality_reduction/pca_samples}
\par\end{centering}

\caption{Amostras geradas utilizando um distribuição normal multivariada.\label{fig:fundamentation:reduction:pca:samples}}



\fonte{Elaborada pelo autor}
\end{figure}


\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/dimensionality_reduction/pca_samples_transformd}
\par\end{centering}

\caption{Amostras reduzidas para o espaço bidimensional utilizando os dois
principais componentes.\label{fig:fundamentation:reduction:pca:samples_transformed}}



\fonte{Elaborada pelo autor}

\end{figure}



\subsection{\emph{Linear Discriminant Analysis}}

\emph{Linear Discriminant Analysis }(LDA) é uma técnica similar ao
PCA, onde buscamos combinações lineares da características para formar
um novo espaço de dimensões reduzidas. A diferênça é que ao contrário
do PCA que busca combinações com maior variância entre os dados, o
LDA procura combinações que melhor separam as classes de um determinado
problema, portanto podemos dizer que o LDA é uma técnica supervisionada,
pois precisamos conhecer as classes que cada índividuo pertence.

Normalmente é utilizado para redução de dimensionalidade, como um
pré-processamento de técnicas de aprendizado de máquina e reconhecimento
de padrões\cite{raschkaLDA}. Também é possível utilizar o LDA como
um classificador. Originalmente proposto em \cite{fisher1936use}como
um problema de \emph{2-class}, que foi generalizado para um problema
\emph{multi-class }em \cite{rao1948utilization}.

Considerando um conjunto de observações $X$ de dimensões $k\times p$,
em que $k$ corresponde à quantidade de indivíduos e $p$ à quantidade
de características de cada indivíduo. Para cada indivíduo em $X$,
existe um elemento em $y$ que indica a classe ao qual pertence. O
objetivo é encontrar uma transformação $W$, que maximize a distância
entre classes (Figura \ref{fig:fundamentation:reduction:lda:bc})
e minimize a distância intra-classe (Figura \ref{fig:fundamentation:reduction:lda:wc}). 

\begin{figure}
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/dimensionality_reduction/wc}
\par\end{centering}

\caption{Distância intra-classe.\label{fig:fundamentation:reduction:lda:wc}}



\fonte{Elaborada pelo autor}

\end{figure}


\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.45]{figures/fundamentation/dimensionality_reduction/bc}\caption{Distancia entre-classes. \label{fig:fundamentation:reduction:lda:bc}}

\par\end{centering}


\fonte{Elaborada pelo autor}
\end{figure}


Dessa forma$W$ mapeia $X$ em $\bar{X}$ através da Equação \ref{eq:fundamentation:reduction:lda:map}

\begin{equation}
\bar{X}=W^{T}\cdot X\label{eq:fundamentation:reduction:lda:map}
\end{equation}
onde $W$ é formado pelos $m$ autovetores com maiores autovalores
absolutos da matriz $S$ dada pela Equação \ref{eq:fundamentation:reduction:lda:sigma}
. Uma observação importante é que $m<c-1$, onde $c$ é a quantidade
de classes que existem em $y$ \cite{raschkaLDA}. 

\begin{equation}
S=S_{W}^{-1}\cdot S_{B}\label{eq:fundamentation:reduction:lda:sigma}
\end{equation}


A matriz $S_{W}$ é o que chamamos de matriz de dispersão intra-classe
e é definida por:

\begin{equation}
S_{W}=\sum_{i=1}^{c}S_{i}\label{eq:fundamentation:reduction:lda:wc}
\end{equation}


\begin{equation}
S_{i}=\sum_{x\in c_{i}}^{n}(x-\mu_{i})\cdot(x-\mu_{i})^{T}\label{eq:fundamentation:reduction:lda:scatter_every_class}
\end{equation}


\begin{equation}
\mu_{i}=\frac{1}{n_{i}}\sum_{x\in c_{i}}^{n}x_{k}\label{eq:fundamentation:reduction:lda:mean_vector_class}
\end{equation}
onde, $S_{i}$ é matriz de dispersão da classe $c_{i}$ e $\mu_{i}$
é o vetor que representa os valores médios de cada característica
de indivíduos que pertecem a classe $c_{i}$.

A matriz $S_{B}$ é chamada de matriz de dispersão entre-classes e
é definida por:

\begin{equation}
S_{B}=\sum_{i=1}^{c}N_{i}(\mu_{i}-\mu)\cdot(\mu_{i}-\mu)^{T}\label{eq:fundamentation:reduction:lda:bc}
\end{equation}
onde, $N_{i}$ é a quantidade de indivíduos que pertecem a classe
$c_{i}$ e $\mu$é o vetor que representa a média das características
de todos os indivíduos em $X$. 

A vantagem de utilizar o LDA é que simplificamos o problema de classificação
criando novas características, a partir das originais, que melhor
separam as classes do conjunto de dados. A grande desvantagem é que
não é possível saber quais características originais tem maior peso,
pois os discriminantes são formados a partir de combinações lineares
de todas elas. A Figura \ref{fig:fundamentation:reduction:lda:samples}
apresenta um conjunto de indivíduos que pertecem a três classes distintas.
Cada classe foi gerada a partir de uma distribuição normal multivariada,
onde os parâmetros diferem entre elas. A Figura \ref{fig:fundamentation:reduction:lda:samples_transformed}
apresenta o resultado da aplicação do LDA para efetuar a redução do
espaço tridimensional para bidimensional.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/dimensionality_reduction/lda_samples}\caption{Amostras geradas de distribuções normais multivariadas.\label{fig:fundamentation:reduction:lda:samples}}

\par\end{centering}


\fonte{Elaborada pelo autor}
\end{figure}


\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/dimensionality_reduction/lda_samples_transformed}
\par\end{centering}

\caption{Resultado da aplicação do LDA para reduzir as amostras da Figura \ref{fig:fundamentation:reduction:lda:samples}.
\label{fig:fundamentation:reduction:lda:samples_transformed}}



\fonte{Elaborada pelo autor}
\end{figure}



\section{\emph{Support Vector Machines}}

Problemas de classificação geralmente envolvem classificar se um certo
indivíduo pertence à classe $A$,$B$ ou $C$. Atualmente existem
diversas técnicas de aprendizado de máquina capazes de realizar essa
tarefa com êxito e umas das principais técnicas utilizadas hoje em
dia são as \emph{Support Vector Machines} (SVM). Foram originalmente
propostas por \cite{cortes1995support} como uma classificador binário,
capaz de dizer se um determinado indivíduo pertence à classe $A$
ou $B$. São parte do grupo de técnicas de aprendizado supervisionado,
onde é necessário conhecer previamente um conjunto de indivíduos para
que o algoritmo gere um modelo que seja capaz de predizer qual a classe
uma nova entrada pertence.

Uma SVM constrói hiperplanos em um espaço de alta dimensão, que pode
ser utilizado para a tarefa de classificação. Uma boa separação é
obtida através do hiperplano que possui maior distância entre os pontos
mais próximos de cada classe no conjunto de dados de treino \cite{sklearn-svm}. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/fundamentation/svm/plot_separating_hyperplane}
\par\end{centering}

\caption{Construção de hiperplanos.\label{fig:fundamentation:svm:hiperplanos}}



\fonte{\cite{sklearn-svm}}
\end{figure}


De acordo com \cite{giovannitcc}, dado $(x_{k},y_{k})$ o conjunto
de amostras para treinamento, sendo que $x_{i}\in\mathbb{R}^{n}$
correspondente ao vetor de características do indivíduo $i$, $y_{i}\in\{1,-1\}$
corresponde à classe do indivíduo $i$, sendo $i=1,2,...,k$. O objetivo
do problema de classificação é encontrar uma função $f(x):\mathbb{R}^{n}\rightarrow\{1,-1\}$
que seja capaz de estimar corretamente a classe do indivíduo $x$.

Na etapa de treinamento a função $f(x)=(w\cdot x)+b$ é estimada,
de forma que a seguinte relação seja satisfeita:

\begin{equation}
y_{i}((w\cdot x_{i})+b)\ge1\label{eq:fundamentation:svm:relation}
\end{equation}
onde $w$ é o vetor normal ao hiperplano e $b$ a distância da função
$f$ em relação à origem. Os valores ótimos de $w$ e $b$ são encontrados
de acordo com a restrição dada pela Equação \ref{eq:fundamentation:svm:relation}
ao minimizar a equação:
\begin{equation}
\phi(w)=\frac{w^{2}}{2}\label{eq:fundamentation:svm:min}
\end{equation}


O SVM possibilita encontrar um hiperplano que minimize a ocorrência
de erros nos casos em que a separação ótima entre as classes não seja
possível. Com o advento de variáveis de folga, é possível relaxar
a restrição da Equação \ref{eq:fundamentation:svm:relation}. Dessa
forma o SVM resolve problema de otimização:

\begin{equation}
min\:\phi(w,\zeta)=\frac{w^{2}}{2}C\sum_{i=1}^{N}\zeta_{i}\label{eq:fundamentation:svm:problem}
\end{equation}
sujeito à:

\begin{equation}
y_{i}((w\cdot x_{i})+b)+\zeta_{i}\ge1\label{eq:fundamentation:svm:relaxed}
\end{equation}
onde $C$ é um parâmetro de treinamento que define o equilíbrio entre
a complexicidade do modelo e o erro de treinamento.

Utilizando a teoria do multiplicadores de Lagrange é possível obter:

\begin{equation}
L(a)=\sum_{i=1}^{N}a_{i}-\frac{1}{2}\sum_{i,j=1}^{N}a_{i}a_{j}y_{i}y_{j}(x_{i},x_{j})\label{eq:fundamentation:svm:lagrange}
\end{equation}


Assim, o objetivo passar a ser encontrar os multiplicadores de Lagrange
$a_{i}$ ótimos que satisfação a Equação \ref{eq:fundamentation:svm:lagrange2}
\cite{chaves2006extraccao}.

\begin{equation}
\sum_{i=1}^{N}a_{i}y_{i}=0,\:0\le a_{i}\le C\label{eq:fundamentation:svm:lagrange2}
\end{equation}


Apenas os pontos onde a restrição imposta pela Equação \ref{eq:fundamentation:svm:relation}
é igual$1$, tem correspondentes $a_{i}\neq0$. Esses pontos são chamados
de vetores de suporte, e estão geometricamente sobre as margens, possuindo
grande importância na definição do hiperplano ótimo, pois delimitam
as margens do conjunto de treinamento. Na Figura \ref{fig:fundamentation:svm:vetores},
os vetores de suporte são representados por circulos circuscritos.

\begin{figure}
\begin{centering}
\includegraphics{figures/fundamentation/svm/vetores}
\par\end{centering}

\caption{Vetores de suporte.\label{fig:fundamentation:svm:vetores}}



\fonte{Elaborada pelo autor}

\end{figure}


Pontos além da margem não tem influência suficiente para determinar
o hiperplano, porém esses vetores de suporte são essenciais nessa
tarefa.

Agora considere o conjunto de amostras da Figura \ref{fig:fundamentation:svm:data_nao_linear}.
Vemos que não é possível traçar uma reta capaz de separar as classes,
tornando o problema um caso não-linear. De acordo com \cite{giovannitcc},
necessitamos de uma transformação não-linear capaz de mapear o conjunto
original (espaço dados) para um novo espaço (espaço de características).
Esse novo espaço deve apresentar dimensões suficientes para que seja
possível realizar a separação linear do conjunto de dados. Dessa forma,
o hiperplano de separação é definido como uma função linear de vetores
retirados do espaço de características e não do espaço de dados. A
construção desse conjunto depende de uma função \emph{$K$}, chamada
de \emph{kernel} \cite{haykin2001redes}. A Equação \ref{eq:fundamentation:svm:lagrange3}
apresenta a forma modificada da \ref{eq:fundamentation:svm:lagrange2}
utilizando uma função $K$.

\begin{figure}[H]
\begin{centering}
\includegraphics{figures/fundamentation/svm/nao_linear}
\par\end{centering}

\caption{Conjunto de indivíduos não separáveis linearmente.\label{fig:fundamentation:svm:data_nao_linear}}



\fonte{Elaborada pelo autor}
\end{figure}


\begin{equation}
L(a)=\sum_{i=1}^{N}a_{i}-\frac{1}{2}\sum_{i,j=1}^{N}a_{i}a_{j}y_{i}y_{j}K(x_{i},x_{j})\label{eq:fundamentation:svm:lagrange3}
\end{equation}


Um conjunto de funções amplamente utilizadas em conjunto com o SVM
para classificação de dados não linearmente separavéis sãos as funções
de base radial (RBF). Uma função RBF é definida por:

\begin{equation}
K(x_{i},y_{i})=exp(-\gamma||x_{i}-y_{i}||^{2})\label{eq:fundamentation:svm:rbf}
\end{equation}
onde $\gamma=1/\sigma^{2}$, onde $\sigma$ é a variância.


\chapter{Metodologia \label{sec:metodologia}}

Nesta seção, são apresentados os procedimentos propostos para a realização
dos objetivos descritos na Seção \ref{sec:objectives}. A Figura \ref{fig:metothology:fluxogram}
apresenta o fluxo das etapas executadas. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/methodology/fluxograma_metodologia}
\par\end{centering}

\begin{centering}
\caption{Fluxograma da metodologia.\label{fig:metothology:fluxogram}}

\par\end{centering}


\fonte{Elaborada pelo autor}

\end{figure}



\section{Aquisição de Imagens}

A aquisição de imagens é uma etapa crucial na metodologia proposta,
pois sem imagens para casos de teste não é possível validar a metodologia.
Atualmente existem poucos bancos de imagem termográfias mamárias disponíveis
para o público geral, sendo a maioria deles de propósito privado.
As imagens utilizadas neste trabalho são provinientes do \emph{Database
for Mastology Research with Infrared Image }- DMR-IR, acessível através
da interface on-line \url{httt://visual.ic.uff.br/dmi}. Mais informações
sobre a base de imagens utilizada podem ser encontradas em \cite{silvaasmd}
e \cite{silva2014new}.

De acordo com \cite{silva2014new}, no protocolo de termografias dinâmicas
as pacientes são submetidas à um estresse térmico causado pela refrigeração
por um ventilador elétrico. Quando a média de temperatura entre as
mamas é de 30.5ºC (Figura \ref{fig:methodology:acquisition:mean_monitoring}),
ou 5 minutos de estresse foram aplicados, a refrigeração é interrompida
e a aquisição sequencial das imagens é iniciada, extraíndo um total
de 20 imagens sequenciais com intervalos de tempo fixos.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{figures/methodology/protocol_mean}
\par\end{centering}

\begin{centering}
\caption{Monitoramento da temperatura média para começar a aquisição sequencial.\label{fig:methodology:acquisition:mean_monitoring}}

\par\end{centering}


\fonte{\cite{silva2014new}}

\end{figure}


As imagens utilizadas possuem dimensão de 640 pixels de largura e
480 pixels de largura e foram utilizados os valores de temperatura
(em graus celsius) obtidos diretamente da câmera termográfica utilizada
na aquisição. A Figura \ref{fig:methodology:acquisition:matrix} apresenta
essas matrizes são armazenadas em formato \emph{.txt}, onde cada exame
é composto por 20 arquivos separados por tempo de acquisição. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.75]{figures/methodology/temp_matrix}
\par\end{centering}

\begin{centering}
\caption{Matriz de temperatura.\label{fig:methodology:acquisition:matrix}}

\par\end{centering}


\fonte{\cite{lincoln_thesis2015}}

\end{figure}


No total foram utilizados 70 exames previamente diagnósticados para
aplicação desta metodologia, sendo que 35 são de pacientes saudáveis
e os 35 restantes de pacientes que apresentam algum tipo de anomalia
nas mamas.


\section{Extração da Região de Interesse}

Essa etapa consiste em extrair a região de interesse (ROI) da imagem.
Como a metodologia tem o objetivo de detectar anomalias no padrão
de vascularização das mamas, é de interesse que as demais regiões
de uma TID sejam excluídas do processo. Alguns métodos de segmentação
automática são encontrados na literatura \cite{marques2012segmentaccao},
porém existem limitações que não tornam possíveis a utilização desse
tipo de técnica. Dessa forma, a região de interesse é extraída de
forma manual, através da utilização de um programa de visualização
e extração de imagens. 

Apesar de cada TID possuir um total de 20 sequências de termogramas,
utilizamos apenas a sequência no tempo inicial para gerar uma máscara
responsável por demarcar a ROI segmentada manualmente através do software
ImageJ \cite{abramoff2004image,rasband1997bethesda}. Utilizando a
máscara da sequência inicial é possível extrair as ROIs das demais
sequências após a aplicação do registro dos termogramas (Seção\emph{
\ref{sec:methodology:image_registration}}). A Figura \ref{fig:methodology:roi:segmentation}
demonstra a etapa de segmentação manual utilizando o mouse para desenhar
o polígono que definirá a ROI. Como resultado uma máscara binária
(Figura \ref{fig:methodology:roi:mask}) é gerada, para ser utilizada
como ROI para todos as 20 sequências de termogramas em um exame. 

\begin{figure}[H]
\hfill{}\subfloat[\label{fig:methodology:roi:segmentation}]{\begin{centering}
\includegraphics[scale=0.5]{figures/methodology/segmentation_click}
\par\end{centering}



}\hfill{}\subfloat[\label{fig:methodology:roi:mask}]{\begin{centering}
\includegraphics[scale=0.5]{figures/methodology/roi_mask}
\par\end{centering}

}\hfill{}

\caption{Extração da região das mamas através do software ImageJ. }



\fonte{\cite{lincoln_thesis2015}}
\end{figure}



\section{Registro das Imagens\label{sec:methodology:image_registration}}

Durante o protocolo de aquisição de imagens, é natural que a paciente
execute movimentos involutários causados pela respiração e ajuste
de postura. Esses movimentos causam diferenças espaciais de uma sequência
para a outra. As Figuras \ref{fig:methodology:image_registration:t1}
e \ref{fig:methodology:image_registration:t2} são termogramas sequênciais
de tempos $t=1$ e $t=2$, respectivamente. A Figura \ref{fig:methodology:image_registration:squared_diff}
apresenta a diferença quadrática $d=(P_{t}(i,j)-P_{t}(i,j))$, onde
$(i,j)$ é a posição do pixels, sendo $i=0,...,639$ e $j=0,...,479$,
e $P_{t}(i,j)$ é o valor de pixel na posição $(i,j)$ no tempo $t$.
É possível notar que existe uma diferença causada pela movimentação
involutária da paciente.

\begin{figure}[H]
\subfloat[\label{fig:methodology:image_registration:t1}]{\begin{centering}
\includegraphics[scale=0.2]{figures/methodology/series1}
\par\end{centering}

}\hfill{}\subfloat[\label{fig:methodology:image_registration:t2}]{\begin{centering}
\includegraphics[scale=0.2]{figures/methodology/series2}
\par\end{centering}



}\hfill{}\subfloat[\label{fig:methodology:image_registration:squared_diff}]{\begin{centering}
\includegraphics[scale=0.2]{figures/methodology/series_diff}
\par\end{centering}



}

\caption{Termogramas dos sequenciais e a diferença de postura entre eles}



\fonte{Elaborada pelo autor.}
\end{figure}


Para analisar as sequências de termogramas de um determinado exame
é necessário corrigir essas diferênças, dessa forma o registro \emph{B-Spline
}apresentado na Seção \ref{sec:fundamentation:image_registration:bspline_registration}
é utilizado. Como dito anteriormente, o registro é uma técnica de
projeção de uma imagem no espaço de coordenadas de outra, ou seja,
ideal para o problema de correção de postura. Utilizando o primeiro
termograma da sequência de um exame como imagem fixa e os termogramas
restantes como imagem móvel, executamos o registro para um determinado
exame 19 vezes, uma para cada imagem móvel. O registro utiliza a Correlação
(Equação X) como métrica de similaridade. O 

\bibliographystyle{plain}
\bibliography{references}

\end{document}
